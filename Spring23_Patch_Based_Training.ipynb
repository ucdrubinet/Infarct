{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb0082bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset, WeightedRandomSampler\n",
    "from torchvision import transforms, utils\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.metrics import classification_report, plot_confusion_matrix\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35026059",
   "metadata": {},
   "source": [
    "Load data for Infarcted and Healthy WSIs at max resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c59e884d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patches(Dataset):\n",
    "    \"\"\"Non-infarcted WSI dataset\"\"\"\n",
    "\n",
    "    def __init__(self, path, cases, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            path (string): Path to the tile folder directory.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.path = path\n",
    "        self.cases = cases\n",
    "        self.WSI = os.listdir(path)\n",
    "        \n",
    "        self.im_paths, self.gt = self.grabTiles()\n",
    "        \n",
    "        self.transform = transform\n",
    "        \n",
    "    def grabTiles(self):\n",
    "        im_paths = []\n",
    "        gt = []\n",
    "        \n",
    "        inf_ct = 0\n",
    "        heal_ct = 0\n",
    "        bg_ct = 0\n",
    "        \n",
    "        for wsi in self.WSI:\n",
    "            heal_ct_case = 0\n",
    "            bg_ct_case = 0\n",
    "            if wsi in self.cases:\n",
    "                for cls in os.listdir(self.path+wsi):\n",
    "                    for image in os.listdir(self.path+wsi+'/'+cls):\n",
    "                        if cls == 'BG' and bg_ct_case <= 300:\n",
    "                            gt.append(0)\n",
    "                            bg_ct += 1\n",
    "                            bg_ct_case += 1\n",
    "                            \n",
    "                            im_path = self.path+wsi+'/'+cls+'/'+image\n",
    "                            im_paths.append(im_path)\n",
    "                        elif cls == 'Heal' and heal_ct_case <= 300:\n",
    "                            gt.append(1)\n",
    "                            heal_ct += 1\n",
    "                            heal_ct_case += 1\n",
    "                            \n",
    "                            im_path = self.path+wsi+'/'+cls+'/'+image\n",
    "                            im_paths.append(im_path)\n",
    "                        elif cls == 'Inf':\n",
    "                            gt.append(2)\n",
    "                            inf_ct += 1\n",
    "                            \n",
    "                            im_path = self.path+wsi+'/'+cls+'/'+image\n",
    "                            im_paths.append(im_path)\n",
    "                            \n",
    "        print(\"Total number of tiles: \")\n",
    "        print(\"Heal = \", heal_ct, \"| Inf = \", inf_ct, \"| BG = \", bg_ct)\n",
    "        \n",
    "        return im_paths, gt\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.im_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = self.im_paths[idx]\n",
    "        cls = self.gt[idx]\n",
    "        \n",
    "        #image = io.imread(img_name)\n",
    "        image = Image.open(img_name)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        sample = [image, cls]\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be988f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomInit(m):\n",
    "    print(\"Model Randomly Initialized\")\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        torch.nn.init.zeros_(m.bias)\n",
    "        \n",
    "\n",
    "def fit(model, loss_fn, optimizer, train_loader, val_loader, num_epochs, scheduler = None, stat_count=5, device=None,PATH = './saved_models/resnet18_Inf.pt'):\n",
    "    curr_model_score = -1\n",
    "    loss_epoch = []\n",
    "    train_epoch = []\n",
    "    val_epoch = []\n",
    "\n",
    "    if device is not None:\n",
    "        model.to(device)\n",
    "    else:\n",
    "        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        model.to(device)\n",
    "\n",
    "    randomInit(model)\n",
    "\n",
    "    if scheduler == None:\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)    \n",
    "    \n",
    "    num_steps = len(train_loader)\n",
    "    \n",
    "    # Iterate through all Epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        if epoch != 0:\n",
    "            scheduler.step()\n",
    "            loss_epoch.append(f_loss.item())\n",
    "            train_epoch.append((total_train_correct/total_train))\n",
    "            val_epoch.append((total_correct/total_val))\n",
    "            \n",
    "        for train_ct in range(num_steps):           \n",
    "            try:\n",
    "                data = next(labelled_iter)\n",
    "            except:\n",
    "                labelled_iter = iter(train_loader)\n",
    "                data = next(labelled_iter)\n",
    "\n",
    "            with torch.enable_grad():\n",
    "                model.train()\n",
    "                images, labels = data[0].to(device,dtype=torch.float), data[1].to(device,dtype=torch.long)\n",
    "                \n",
    "                #print(labels)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images)\n",
    "                \n",
    "                loss = loss_fn(outputs, labels)\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                # Print statistics on every stat_count iteration\n",
    "                if (train_ct+1) % stat_count == 0:\n",
    "                    print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n",
    "                                %(epoch+1, num_epochs, train_ct+1, \n",
    "                                len(train_loader), loss.item()))\n",
    "                \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            \n",
    "            total_val = 0\n",
    "            total_correct = 0\n",
    "            val_predicted_full = []\n",
    "            val_labels_full = []\n",
    "            for data in val_loader:\n",
    "                images, labels = data[0].cuda(), data[1].cuda()\n",
    "                outputs = model(images)\n",
    "\n",
    "                _, val_predicted = torch.max(outputs.data, 1)\n",
    "                total_val += val_labels.size(0)\n",
    "                total_correct += (val_predicted == val_labels).sum().item()\n",
    "\n",
    "                val_predicted_full = val_predicted_full + val_predicted.cpu().data.numpy().tolist()\n",
    "                val_labels_full = val_labels_full + val_labels_full.cpu().data.numpy().tolist()\n",
    "                \n",
    "                \n",
    "            total_train = 0\n",
    "            total_train_correct = 0\n",
    "            train_predicted_full = []\n",
    "            train_labels_full = []\n",
    "            for data in train_loader:\n",
    "                images, labels = data[0].cuda(), data[1].cuda()\n",
    "                outputs = model(images)\n",
    "\n",
    "                _, train_predicted = torch.max(outputs.data, 1)\n",
    "                total_train += train_labels.size(0)\n",
    "                total_train_correct += (train_predicted == train_labels).sum().item()\n",
    "\n",
    "                train_predicted_full = train_predicted_full + train_predicted.cpu().data.numpy().tolist()\n",
    "                train_labels_full = train_labels_full + train_labels_full.cpu().data.numpy().tolist()\n",
    "                \n",
    "                \n",
    "        print(\"END OF EPOCH\")\n",
    "        val_dict = classification_report(val_labels_full, val_predicted_full, labels=[0,1,2],output_dict=True)\n",
    "        train_dict = classification_report(train_labels_full, train_predicted_full, labels=[0,1,2],output_dict=True)\n",
    "\n",
    "        val0_score = val_dict['0']['f1-score']\n",
    "        val1_score = val_dict['1']['f1-score']\n",
    "        val2_score = val_dict['2']['f1-score']\n",
    "        \n",
    "        train0_score = train_dict['0']['f1-score']\n",
    "        train1_score = train_dict['1']['f1-score']\n",
    "        train2_score = train_dict['2']['f1-score']\n",
    "        \n",
    "        print(\"Validation Set: \")\n",
    "        print(\"Model Score = \", (total_correct/total_val)*val0_score*val1_score*val2_score)\n",
    "        print(\"Accuracy = \", (total_correct/total_val))\n",
    "        \n",
    "        print(\"Train Set: \")\n",
    "        print(\"Model Score = \", (total_train_correct/total_train)*train0_score*train1_score*train2_score)\n",
    "        print(\"Accuracy = \", (total_train_correct/total_train))\n",
    "        \n",
    "        if curr_model_score < (total_correct/total_val)*val0_score*val1_score*val2_score: \n",
    "            curr_model_score = (total_correct/total_val)*val0_score*val1_score*val2_score\n",
    "            print(\"Model Checkpoint saved!\")\n",
    "        torch.save({'model_state_dict': model.state_dict()}, PATH)\n",
    "        \n",
    "    try:\n",
    "        plt.plot(loss_epoch,label='Loss')\n",
    "        plt.savefig('loss_plot.png')\n",
    "\n",
    "        plt.plot(train_epoch,label='Train Acc')\n",
    "        plt.plot(val_epoch,label='Val Acc')\n",
    "        plt.savefig('acc_plot.png')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return loss_epoch, train_epoch, val_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed4b3196",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.RandomEqualize(p=0.1),\n",
    "        transforms.RandomAutocontrast(p=0.1),\n",
    "        transforms.RandomAdjustSharpness(sharpness_factor=4,p=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.4409763317567454, 0.4016568471536302, 0.4988298669112181],\n",
    "                             std=[0.31297803931100737, 0.2990562933047881, 0.33747493782548915])\n",
    "    ])\n",
    "\n",
    "# train_transform = transforms.Compose([\n",
    "#         transforms.RandomHorizontalFlip(p=0.5),\n",
    "#         transforms.RandomVerticalFlip(p=0.5),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean=[0.4409763317567454, 0.4016568471536302, 0.4988298669112181],\n",
    "#                              std=[0.31297803931100737, 0.2990562933047881, 0.33747493782548915])\n",
    "#     ])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.4409763317567454, 0.4016568471536302, 0.4988298669112181],\n",
    "                             std=[0.31297803931100737, 0.2990562933047881, 0.33747493782548915])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "070f1980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NA-5029-16_HE',\n",
       " 'NA-5029-17_HE',\n",
       " 'NA-5029-18_HE',\n",
       " 'NA5004-16_HE',\n",
       " 'NA5004-17_HE',\n",
       " 'NA5004-18_HE',\n",
       " 'NA5007-16_HE',\n",
       " 'NA5007-17_HE',\n",
       " 'NA5007-18_HE',\n",
       " 'NA5009-16_HE',\n",
       " 'NA5009-17_HE',\n",
       " 'NA5009-18_HE',\n",
       " 'NA5031-16_HE',\n",
       " 'NA5031-17_HE',\n",
       " 'NA5031-18_HE',\n",
       " 'NA5041-16_HE',\n",
       " 'NA5041-17_HE',\n",
       " 'NA5041-18_HE',\n",
       " 'NA5045-16_HE',\n",
       " 'NA5045-17_HE',\n",
       " 'NA5045-18_HE',\n",
       " 'NA5051-16_HE',\n",
       " 'NA5051-17_HE',\n",
       " 'NA5051-18_HE',\n",
       " 'NA5057-16_HE',\n",
       " 'NA5057-17_HE',\n",
       " 'NA5057-18_HE',\n",
       " 'NA5063-16_HE',\n",
       " 'NA5063-17_HE',\n",
       " 'NA5063-18_HE',\n",
       " 'NA5077-16_HE',\n",
       " 'NA5077-17_HE',\n",
       " 'NA5077-18_HE',\n",
       " 'NA5085-16_HE',\n",
       " 'NA5085-17_HE',\n",
       " 'NA5085-18_HE',\n",
       " 'NA5089-16_HE',\n",
       " 'NA5089-17_HE',\n",
       " 'NA5089-18_HE',\n",
       " 'NA5090-16_HE',\n",
       " 'NA5090-17_HE',\n",
       " 'NA5090-18_HE',\n",
       " 'NA5091-16_HE',\n",
       " 'NA5091-17_HE',\n",
       " 'NA5091-18_HE',\n",
       " 'NA5093-16_HE',\n",
       " 'NA5093-17_HE',\n",
       " 'NA5093-18_HE',\n",
       " 'NA5095-16_HE',\n",
       " 'NA5095-17_HE',\n",
       " 'NA5095-18_HE',\n",
       " 'NA5098-16_HE',\n",
       " 'NA5098-17_HE',\n",
       " 'NA5098-18_HE',\n",
       " 'NA5114-16_HE',\n",
       " 'NA5114-17_HE',\n",
       " 'NA5114-18_HE',\n",
       " 'NA5116-16_HE',\n",
       " 'NA5116-17_HE',\n",
       " 'NA5116-18_HE',\n",
       " 'NA5137-16_HE',\n",
       " 'NA5137-17_HE',\n",
       " 'NA5137-18_HE',\n",
       " 'NA5146-16_HE',\n",
       " 'NA5146-17_HE',\n",
       " 'NA5146-18_HE',\n",
       " 'NA5161-16_HE',\n",
       " 'NA5161-17_HE',\n",
       " 'NA5161-18_HE']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('/cache/S23_Infarct/seg_data_512/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32170d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on train set\n",
      "Total number of tiles: \n",
      "Heal =  15351 | Inf =  10679 | BG =  15351\n",
      "Working on validation set\n",
      "Total number of tiles: \n",
      "Heal =  1204 | Inf =  21 | BG =  1204\n"
     ]
    }
   ],
   "source": [
    "SEGMENTATION_TILE_DIR = '/cache/S23_Infarct/seg_data_512/'\n",
    "all_cases = os.listdir(SEGMENTATION_TILE_DIR)\n",
    "\n",
    "train_cases = []\n",
    "val_cases = ['NA5031-18_HE','NA5041-17_HE','NA5095-17_HE','NA5116-16_HE']\n",
    "\n",
    "test_cases = ['NA-5029-16_HE','NA5093-17_HE','NA5091-16_HE','NA-5029-18_HE','NA5063-17_HE','NA5077-18_HE','NA5089-17_HE',\n",
    "             'NA5095-16_HE','NA5114-16_HE','NA5146-17_HE','NA5077-17_HE','NA5146-18_HE','NA5051-17_HE','NA5007-17_HE']\n",
    "\n",
    "\n",
    "train_cases = list(set(all_cases) - set(test_cases))\n",
    "train_cases = list(set(train_cases) - set(val_cases))\n",
    "\n",
    "print(\"Working on train set\")\n",
    "trainset = Patches(SEGMENTATION_TILE_DIR,train_cases,train_transform)\n",
    "\n",
    "print(\"Working on validation set\")\n",
    "valset = Patches(SEGMENTATION_TILE_DIR,val_cases,test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7dd9ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 40\n",
    "num_epochs = 100\n",
    "\n",
    "loss_f = nn.CrossEntropyLoss()\n",
    "\n",
    "model = torchvision.models.resnet50()\n",
    "model.fc = nn.Linear(2048, 3)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26104ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(valset,batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07eae8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Randomly Initialized\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 160.00 MiB (GPU 0; 15.75 GiB total capacity; 14.66 GiB already allocated; 38.31 MiB free; 14.83 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-11a18d4eb03d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mPATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./saved_models/resnet50_512.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-ccabd345d390>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, loss_fn, optimizer, train_loader, val_loader, num_epochs, scheduler, stat_count, device, PATH)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/vips/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/vips/lib/python3.7/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/vips/lib/python3.7/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/vips/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/vips/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/vips/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/vips/lib/python3.7/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/vips/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/vips/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/vips/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    344\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    345\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 346\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 160.00 MiB (GPU 0; 15.75 GiB total capacity; 14.66 GiB already allocated; 38.31 MiB free; 14.83 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "loss_epoch, train_epoch, val_epoch = fit(model, loss_f, optimizer, train_loader, val_loader, num_epochs,PATH = './saved_models/resnet50_512.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedd0119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0344b51d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
