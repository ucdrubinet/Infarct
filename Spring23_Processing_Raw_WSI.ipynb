{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87a990ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Luca: We must tile all the WSI before proceeding with either segmentation or WSI-level. This notebook does the preprocessing \n",
    "\n",
    "\n",
    "#Segmentation Pipeline\n",
    "    # Tile\n",
    "    # Give tile name based on coordinate\n",
    "    # Extract polygon annotation\n",
    "    # Generate Masks based on Polygon Annotation\n",
    "    # Find BG using ../saved_models/patched_512_NewAnnotation+preprocess_ResNet18_Feb12.pt (in 224)\n",
    "    # Separate tiles into BG, Inf and Healthy folders\n",
    "#WSI-level Pipeline\n",
    "    # Tile\n",
    "    # Remove BG tiles using ../saved_models/patched_512_NewAnnotation+preprocess_ResNet18_Feb12.pt (in 224)\n",
    "        # and name them sequentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "529cc090",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#import os.path\n",
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyvips as Vips\n",
    "from tqdm import tqdm\n",
    "from utils import vips_utils, normalize\n",
    "from torchvision import transforms, utils\n",
    "import time\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image, ImageFile\n",
    "import statistics\n",
    "from typing import Optional, Tuple\n",
    "import pylibczi\n",
    "from pylibczi import CziScene\n",
    "import czifile\n",
    "from czifile import CziFile \n",
    "import xml.etree.ElementTree as ET\n",
    "import argparse\n",
    "import gc \n",
    "import psutil\n",
    "import resource\n",
    "import platform\n",
    "import pickle\n",
    "import xmltodict\n",
    "import time\n",
    "import matplotlib.path as mpath\n",
    "from skimage.draw import polygon\n",
    "import cv2\n",
    "import copy\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from skimage import io, transform\n",
    "from skimage.morphology import convex_hull_image\n",
    "\n",
    "### PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25e94be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "TILE_SIZE = 512\n",
    "\n",
    "WSI_DIR = '/cache/Inf_May23_dataset/'\n",
    "SAVE_DIR = '/cache/S23_Infarct/patched_'+str(TILE_SIZE)+'/'\n",
    "CZ_DIR = '/cache/S23_Infarct/annotation/'\n",
    "MASK_DIR = '/cache/S23_Infarct/masks/'\n",
    "\n",
    "SEGMENTATION_TILE_DIR = '/cache/S23_Infarct/seg_data_'+str(TILE_SIZE)+'/'\n",
    "WSI_TILE_DIR = '/cache/S23_Infarct/wsi_level_data_'+str(TILE_SIZE)+'/'\n",
    "\n",
    "tile_czi = True\n",
    "tile_svs = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "099e84bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tile folder you provided us does not exist, being created now...\n",
      "Tile folder for WSI-level you provided us does not exist, being created now...\n",
      "Tile folder you provided us does not exist, being created now...\n",
      "Mask folder you provided us does not exist, being created now...\n",
      "Found WSI folder... \n",
      "All WSIs in wsi_dir: \n",
      "['NA-5029-16_HE.svs', 'NA-5029-17_HE.svs', 'NA-5029-18_HE.svs', 'NA5004-16_HE.svs', 'NA5004-17_HE.svs', 'NA5004-18_HE.svs', 'NA5007-16_HE.svs', 'NA5007-17_HE.svs', 'NA5007-18_HE.svs', 'NA5009-16_HE.svs', 'NA5009-17_HE.svs', 'NA5009-18_HE.svs', 'NA5031-16_HE.svs', 'NA5031-17_HE.svs', 'NA5031-18_HE.svs', 'NA5041-16_HE.svs', 'NA5041-17_HE.svs', 'NA5041-18_HE.svs', 'NA5045-16_HE.svs', 'NA5045-17_HE.svs', 'NA5045-18_HE.svs', 'NA5051-16_HE.czi', 'NA5051-17_HE.czi', 'NA5051-18_HE.czi', 'NA5057-16_HE.svs', 'NA5057-17_HE.svs', 'NA5057-18_HE.svs', 'NA5063-16_HE.czi', 'NA5063-17_HE.czi', 'NA5063-18_HE.czi', 'NA5077-16_HE.czi', 'NA5077-17_HE.czi', 'NA5077-18_HE.czi', 'NA5085-16_HE.czi', 'NA5085-17_HE.czi', 'NA5085-18_HE.czi', 'NA5089-16_HE.czi', 'NA5089-17_HE.czi', 'NA5089-18_HE.czi', 'NA5090-16_HE.czi', 'NA5090-17_HE.czi', 'NA5090-18_HE.czi', 'NA5091-16_HE.czi', 'NA5091-17_HE.czi', 'NA5091-18_HE.czi', 'NA5093-16_HE.czi', 'NA5093-17_HE.czi', 'NA5093-18_HE.czi', 'NA5095-16_HE.czi', 'NA5095-17_HE.czi', 'NA5095-18_HE.czi', 'NA5098-16_HE.czi', 'NA5098-17_HE.czi', 'NA5098-18_HE.czi', 'NA5114-16_HE.czi', 'NA5114-17_HE.czi', 'NA5114-18_HE.czi', 'NA5116-16_HE.czi', 'NA5116-17_HE.czi', 'NA5116-18_HE.czi', 'NA5137-16_HE.czi', 'NA5137-17_HE.czi', 'NA5137-18_HE.czi', 'NA5146-16_HE.svs', 'NA5146-17_HE.svs', 'NA5146-18_HE.svs', 'NA5161-16_HE.czi', 'NA5161-17_HE.czi', 'NA5161-18_HE.czi']\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(WSI_DIR):\n",
    "    print(\"WSI folder does not exist, script should stop now\")\n",
    "else:\n",
    "    if not os.path.exists(SEGMENTATION_TILE_DIR):\n",
    "        print(\"Tile folder you provided us does not exist, being created now...\")\n",
    "        os.makedirs(SEGMENTATION_TILE_DIR)\n",
    "        \n",
    "    if not os.path.exists(WSI_TILE_DIR):\n",
    "        print(\"Tile folder for WSI-level you provided us does not exist, being created now...\")\n",
    "        os.makedirs(WSI_TILE_DIR)\n",
    "\n",
    "    if not os.path.exists(SAVE_DIR):\n",
    "        print(\"Tile folder you provided us does not exist, being created now...\")\n",
    "        os.makedirs(SAVE_DIR)\n",
    "\n",
    "    if not os.path.exists(CZ_DIR):\n",
    "        print(\"Annotation folder you provided us does not exist, being created now...\")\n",
    "        os.makedirs(CZ_DIR)\n",
    "\n",
    "    if not os.path.exists(MASK_DIR):\n",
    "        print(\"Mask folder you provided us does not exist, being created now...\")\n",
    "        os.makedirs(MASK_DIR)\n",
    "\n",
    "    print(\"Found WSI folder... \")\n",
    "    wsi_slides = os.listdir(WSI_DIR)\n",
    "    imagenames = sorted(wsi_slides)\n",
    "    print(\"All WSIs in wsi_dir: \")\n",
    "    print(imagenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6772d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grabCZI(path, verbose = False):\n",
    "    img = czifile.imread(path)\n",
    "    if verbose:\n",
    "        print(img.shape)\n",
    "        print(img)\n",
    "    \n",
    "    img = np.array(img, dtype = np.uint8)\n",
    "    \n",
    "    scenes = img.shape[0]\n",
    "    time = img.shape[1]\n",
    "    height = img.shape[2]\n",
    "    width = img.shape[3]\n",
    "    channels = img.shape[4]\n",
    "    \n",
    "    \n",
    "    img = img.reshape((height, width, channels))\n",
    "    if verbose:\n",
    "        print(img)\n",
    "        print(img.shape) \n",
    "        \n",
    "    dtype_to_format = {\n",
    "        'uint8': 'uchar',\n",
    "        'int8': 'char',\n",
    "        'uint16': 'ushort',\n",
    "        'int16': 'short',\n",
    "        'uint32': 'uint',\n",
    "        'int32': 'int',\n",
    "        'float32': 'float',\n",
    "        'float64': 'double',\n",
    "        'complex64': 'complex',\n",
    "        'complex128': 'dpcomplex',\n",
    "    }\n",
    "    \n",
    "    ###codes from numpy2vips\n",
    "    height, width, bands = img.shape\n",
    "    img = img.reshape(width * height * bands)\n",
    "    vips = Vips.Image.new_from_memory(img.data, width, height, bands,\n",
    "                                      dtype_to_format['uint8'])\n",
    "    try: \n",
    "        del img, height, width, bands\n",
    "        gc.collect()\n",
    "    except: \n",
    "        pass\n",
    "    \n",
    "    return vips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f781e3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting tiling....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/69 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading NA-5029-16_HE.svs  ......\n",
      "Pre resize:  34033 x 45816\n",
      "Resizing small case\n",
      "Post resize:  68066 x 91632\n",
      "Loaded Image: /cache/Inf_May23_dataset/NA-5029-16_HE.svs\n",
      "/cache/S23_Infarct/patched_512/NA-5029-16_HE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 1/69 [01:12<1:22:37, 72.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Tiling:  /cache/Inf_May23_dataset/NA-5029-16_HE.svs\n",
      "processed in  72.91144609451294  seconds\n",
      "____________________________________________\n",
      "Loading NA-5029-17_HE.svs  ......\n",
      "Pre resize:  33509 x 53784\n",
      "Resizing small case\n",
      "Post resize:  67018 x 107568\n",
      "Loaded Image: /cache/Inf_May23_dataset/NA-5029-17_HE.svs\n",
      "/cache/S23_Infarct/patched_512/NA-5029-17_HE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2/69 [02:37<1:28:55, 79.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Tiling:  /cache/Inf_May23_dataset/NA-5029-17_HE.svs\n",
      "processed in  84.34032082557678  seconds\n",
      "____________________________________________\n",
      "Loading NA-5029-18_HE.svs  ......\n",
      "Pre resize:  41428 x 47808\n",
      "Resizing small case\n",
      "Post resize:  82856 x 95616\n",
      "Loaded Image: /cache/Inf_May23_dataset/NA-5029-18_HE.svs\n",
      "/cache/S23_Infarct/patched_512/NA-5029-18_HE\n"
     ]
    }
   ],
   "source": [
    "###### SEGMENTATION / WSI-level Pipeline - Tile\n",
    "\n",
    "print(\"Starting tiling....\")\n",
    "for imagename in tqdm(imagenames[:]):\n",
    "    start = time.time()\n",
    "    if imagename.split('.')[-1] == 'svs':\n",
    "        NAID = imagename.split('.')[0]\n",
    "        print(\"Loading\", imagename, \" ......\")\n",
    "        vips_img = Vips.Image.new_from_file(WSI_DIR + imagename, level=0)\n",
    "        \n",
    "        print(\"Pre resize: \", vips_img.height, \"x\", vips_img.width)\n",
    "        \n",
    "        if NAID in ['NA5009-16_HE', 'NA5009-17_HE', 'NA5009-18_HE']:\n",
    "            pass\n",
    "            #vips_img = vips_img.resize(0.25)\n",
    "        else:\n",
    "            print(\"Resizing small case\")\n",
    "            vips_img = vips_img.resize(2)\n",
    "            \n",
    "        print(\"Post resize: \", vips_img.height, \"x\", vips_img.width)\n",
    "            \n",
    "        print(\"Loaded Image: \" + WSI_DIR + imagename)\n",
    "\n",
    "        if tile_svs:\n",
    "        \n",
    "            vips_utils.save_and_tile(vips_img, os.path.splitext(imagename)[0] \\\n",
    "                                     , SAVE_DIR, tile_size = TILE_SIZE)\n",
    "            print(\"Done Tiling: \", WSI_DIR + imagename)\n",
    "        \n",
    "    elif imagename.split('.')[-1] == 'czi':\n",
    "        NAID = imagename.split('.')[0]\n",
    "        print(\"Loading\", imagename, \" ......\")\n",
    "        try: \n",
    "            vips_img = grabCZI(WSI_DIR + imagename)\n",
    "            print(\"Loaded Image: \" + WSI_DIR + imagename)\n",
    "            \n",
    "            print(\"Pre resize: \", vips_img.height, \"x\", vips_img.width)\n",
    "            \n",
    "            if tile_czi:\n",
    "        \n",
    "                vips_utils.save_and_tile(vips_img, os.path.splitext(imagename)[0] \\\n",
    "                                         , SAVE_DIR, tile_size = TILE_SIZE)\n",
    "\n",
    "                print(\"Done Tiling: \", WSI_DIR + imagename)\n",
    "                del vips_img\n",
    "                gc.collect()\n",
    "                print(\"Finish Delete\", WSI_DIR + imagename)\n",
    "        except:\n",
    "            print(\"Error in tiling\")\n",
    "        \n",
    "        \n",
    "\n",
    "    else:\n",
    "        print(\"Skipped,\", imagename, '. This file is either not .czi or .svs, or not the file assigned')\n",
    "    \n",
    "    try: \n",
    "        del vips_img \n",
    "        gc.collect()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    print(\"processed in \", time.time()-start,\" seconds\")\n",
    "    print(\"____________________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26675b34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###### SEGMENTATION - Give tile name based on coordinate\n",
    "\n",
    "print(\"About to change names to add coordinates\")\n",
    "for case_folder in sorted(os.listdir(SAVE_DIR)):\n",
    "    NAID = case_folder\n",
    "    print('Processing NAID: ', NAID)\n",
    "    for tile_folder in sorted(os.listdir(SAVE_DIR+NAID+'/0/')):\n",
    "        # folder_level == y axis distance determinant\n",
    "        # file_level == x axis distance determinant\n",
    "        y0 = int(tile_folder)*TILE_SIZE\n",
    "        y1 = (int(tile_folder)+1)*TILE_SIZE\n",
    "        #print(\"Y axis = (\", y0,',',y1,')')\n",
    "        for tile_file in sorted(os.listdir(SAVE_DIR+NAID+'/0/'+tile_folder+'/')):\n",
    "            x0 = int(tile_file.split('.')[0])*TILE_SIZE\n",
    "            x1 = (int(tile_file.split('.')[0])+1)*TILE_SIZE\n",
    "            #print(\"X axis = (\", x0,',',x1,')')\n",
    "            #print(\"Renaming \", SAVE_DIR+NAID+'/0/'+str(tile_folder)+'/'+tile_file)\n",
    "            os.rename(SAVE_DIR+NAID+'/0/'+str(tile_folder)+'/'+tile_file, \n",
    "                      SAVE_DIR+NAID+'/0/'+str(tile_folder)+'/'+str(y0)+'_'+str(x0)+'_'+str(y1)+'_'+str(x1)+'_'+tile_file)\n",
    "            #print(\"Renamed into \", SAVE_DIR+NAID+'/0/'+tile_folder+'/'+str(y0)+'_'+str(x0)+'_'+str(y1)+'_'+str(x1)+'_'+t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f40863",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### SEGMENTATION - Extract polygon annotation\n",
    "\n",
    "for imagename in tqdm(imagenames[:]):\n",
    "    start = time.time()\n",
    "    if imagename.split('.')[-1] == 'svs':\n",
    "        pass\n",
    "        \n",
    "    elif imagename.split('.')[-1] == 'czi':\n",
    "        NAID = imagename.split('.')[0]\n",
    "        print(\"Loading\", imagename, \" ......\")\n",
    "        try: \n",
    "            czifile = pylibczi.CziFile(WSI_DIR + imagename, metafile_out = CZ_DIR + NAID + '.cz',use_pylibczi=True, verbose=True)\n",
    "            czifile.read_meta()\n",
    "            \n",
    "            tree = ET.parse(CZ_DIR + NAID + '.cz') \n",
    "            root = tree.getroot() \n",
    "            \n",
    "            tree.write(CZ_DIR + NAID + '.xml')\n",
    "        except:\n",
    "            print(\"Error in extracting annotation\")\n",
    "            \n",
    "    print(\"processed in \", time.time()-start,\" seconds\")\n",
    "    print(\"____________________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1796e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_svs_annot(path):\n",
    "    doc = xmltodict.parse(open(path, 'r', encoding='utf-8').read())\n",
    "    \n",
    "    all_annot = []\n",
    "    if type(doc['Annotations']['Annotation']['Regions']['Region']) == list:\n",
    "        for region_idx in range(len(doc['Annotations']['Annotation']['Regions']['Region'])):\n",
    "            coords = []\n",
    "            for annot in doc['Annotations']['Annotation']['Regions']['Region'][region_idx]['Vertices']['Vertex']:\n",
    "                coord = [int(annot['@Y']),int(annot['@X'])]\n",
    "\n",
    "                coords.append(coord)\n",
    "            all_annot.append(coords)\n",
    "    else:\n",
    "        coords = []\n",
    "        for annot in doc['Annotations']['Annotation']['Regions']['Region']['Vertices']['Vertex']:\n",
    "            coord = [int(annot['@Y']),int(annot['@X'])]\n",
    "\n",
    "            coords.append(coord)\n",
    "        all_annot.append(coords)\n",
    "    \n",
    "    return all_annot\n",
    "\n",
    "#from: https://stackoverflow.com/questions/9807634/find-all-occurrences-of-a-key-in-nested-dictionaries-and-lists\n",
    "def findkeys(node, kv):\n",
    "    if isinstance(node, list):\n",
    "        for i in node:\n",
    "            for x in findkeys(i, kv):\n",
    "               yield x\n",
    "    elif isinstance(node, dict):\n",
    "        if kv in node:\n",
    "            yield node[kv]\n",
    "        for j in node.values():\n",
    "            for x in findkeys(j, kv):\n",
    "                yield x\n",
    "\n",
    "def grab_czi_annot(path):\n",
    "    doc = xmltodict.parse(open(path, 'r', encoding='utf-8').read())\n",
    "    \n",
    "    amt_annot = len(list(findkeys(doc,'Points')))\n",
    "    \n",
    "    all_annot = []\n",
    "    coords = []\n",
    "    \n",
    "    #print(\"AMOUNT ANNOT = \", amt_annot)\n",
    "    \n",
    "    for annot_idx in range(amt_annot):\n",
    "        coords = []\n",
    "        all_cord = list(findkeys(doc,'Points'))[annot_idx].split(' ')\n",
    "        isGood = False\n",
    "        for xy in all_cord:\n",
    "            #print(xy)\n",
    "            if int(float(xy.split(',')[1])) < 0 or int(float(xy.split(',')[0])) < 0:\n",
    "                pass\n",
    "                #print(\"Passed on negative coordinates\")\n",
    "            else:\n",
    "                coord = [int(float(xy.split(',')[1])),int(float(xy.split(',')[0]))]\n",
    "                isGood = True\n",
    "                \n",
    "                coords.append(coord)\n",
    "        if isGood:\n",
    "            all_annot.append(coords)\n",
    "    \n",
    "    return all_annot\n",
    "\n",
    "\n",
    "\n",
    "def create_polygon_mask(vertex_coordinates, mask):\n",
    "    \n",
    "    rr, cc = polygon(vertex_coordinates[:,0],vertex_coordinates[:,1],mask.shape)\n",
    "    mask[rr,cc] = 1\n",
    "    \n",
    "    return mask\n",
    "\n",
    "\n",
    "# https://stackoverflow.com/questions/41925853/fill-shapes-contours-using-numpy\n",
    "def fill_contours(arr):\n",
    "    return np.maximum.accumulate(arr,1) & \\\n",
    "           np.maximum.accumulate(arr[:,::-1],1)[:,::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd7a81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### SEGMENTATION - Generate Masks based on Polygon Annotation\n",
    "\n",
    "gt_df = pd.read_csv('/cache/S23_Infarct/gt.csv')\n",
    "\n",
    "for imagename in tqdm(imagenames[:]):\n",
    "    start = time.time()\n",
    "\n",
    "    wsi_gt = gt_df[gt_df.cases == imagename].iat[0,2]\n",
    "    \n",
    "    resized = False\n",
    "    if wsi_gt == 1:\n",
    "        if imagename.split('.')[-1] == 'svs':\n",
    "            NAID = imagename.split('.')[0]\n",
    "            print(\"Loading\", imagename, \" ......\")\n",
    "            if True: \n",
    "                vips_img = Vips.Image.new_from_file(WSI_DIR + imagename, level=0)\n",
    "                print(\"Loaded Image: \" + WSI_DIR + imagename)\n",
    "                \n",
    "                if NAID in ['NA5009-16_HE', 'NA5009-17_HE', 'NA5009-18_HE']:\n",
    "                    pass\n",
    "                    #vips_img = vips_img.resize(0.25)\n",
    "                else:\n",
    "                    print(\"Resizing small case\")\n",
    "                    vips_img = vips_img.resize(2)\n",
    "                    resized = True\n",
    "\n",
    "                dimension = [vips_img.height, vips_img.width]\n",
    "\n",
    "                all_annot = grab_svs_annot(CZ_DIR + NAID + '.xml')\n",
    "\n",
    "                mask = np.zeros(dimension,'uint8')\n",
    "                \n",
    "                # It is all matching xml up to here\n",
    "                #print(all_annot)\n",
    "                \n",
    "                print(\"Starting to build mask\")\n",
    "                print(\"Number of annotation groups =\", len(all_annot))\n",
    "                for coords in all_annot:\n",
    "                    #Old method - faulty\n",
    "                    #mask = create_polygon_mask(coords, mask)\n",
    "                    \n",
    "                    #New method\n",
    "                    max_y = 0\n",
    "                    max_x = 0\n",
    "                    min_y = 10000000000\n",
    "                    min_x = 10000000000\n",
    "                    \n",
    "                    submask = np.zeros(dimension,'uint8')\n",
    "                    for coord in coords:\n",
    "                        y = coord[0]\n",
    "                        x = coord[1]\n",
    "                        \n",
    "                        #print(\"y = \", y, \"x = \", x)\n",
    "                        if resized:\n",
    "                            y = y*2\n",
    "                            x = x*2\n",
    "                            submask[y][x] = 1\n",
    "                        else:\n",
    "                            submask[y][x] = 1\n",
    "                            \n",
    "                        if y > max_y:\n",
    "                            max_y = y\n",
    "                            \n",
    "                        if x > max_x:\n",
    "                            max_x = x\n",
    "                            \n",
    "                        if y < min_y:\n",
    "                            min_y = y\n",
    "                            \n",
    "                        if x < min_x:\n",
    "                            min_x = x\n",
    "                    \n",
    "                    submask[min_y:max_y+1, min_x:max_x+1] = convex_hull_image(submask[min_y:max_y+1, min_x:max_x+1])\n",
    "                    \n",
    "                    mask = mask + submask\n",
    "                        \n",
    "                \n",
    "                #print(\"Filling contours\")\n",
    "                #mask = fill_contours(mask)\n",
    "\n",
    "                #mask = np.packbits(mask,axis=None)\n",
    "                #print(\"Binarized mask\")\n",
    "                \n",
    "                print(\"Saving mask contours\")\n",
    "                np.save(MASK_DIR+NAID+'.npy',mask)\n",
    "\n",
    "                print(\"processed in \", time.time()-start,\" seconds\")\n",
    "                print(\"____________________________________________\")\n",
    "\n",
    "            else:\n",
    "                print(\"Error in generating masks from polygon for \", NAID)\n",
    "\n",
    "\n",
    "        elif imagename.split('.')[-1] == 'czi':\n",
    "            NAID = imagename.split('.')[0]\n",
    "            print(\"Loading\", imagename, \" ......\")\n",
    "            if True: \n",
    "                vips_img = grabCZI(WSI_DIR + imagename)\n",
    "                print(\"Loaded Image: \" + WSI_DIR + imagename)\n",
    "\n",
    "                dimension = [vips_img.height, vips_img.width]\n",
    "                all_annot = grab_czi_annot(CZ_DIR + NAID + '.xml')\n",
    "\n",
    "                mask = np.zeros(dimension,'uint8')\n",
    "                \n",
    "                # It is all matching up to here\n",
    "                #print(all_annot)\n",
    "                \n",
    "                print(\"Starting to build mask\")\n",
    "                print(\"Number of annotation groups =\", len(all_annot))\n",
    "                for coords in all_annot:\n",
    "                    #Old method - faulty\n",
    "                    #mask = create_polygon_mask(coords, mask)\n",
    "                    \n",
    "                    #New method\n",
    "                    max_y = 0\n",
    "                    max_x = 0\n",
    "                    min_y = 10000000000\n",
    "                    min_x = 10000000000\n",
    "                    \n",
    "                    submask = np.zeros(dimension,'uint8')\n",
    "                    for coord in coords:\n",
    "                        y = coord[0]\n",
    "                        x = coord[1]\n",
    "                        \n",
    "                        #print(\"y = \", y, \"x = \", x)\n",
    "                        if resized:\n",
    "                            y = y*2\n",
    "                            x = x*2\n",
    "                            submask[y][x] = 1\n",
    "                        else:\n",
    "                            submask[y][x] = 1\n",
    "                            \n",
    "                        if y > max_y:\n",
    "                            max_y = y\n",
    "                            \n",
    "                        if x > max_x:\n",
    "                            max_x = x\n",
    "                            \n",
    "                        if y < min_y:\n",
    "                            min_y = y\n",
    "                            \n",
    "                        if x < min_x:\n",
    "                            min_x = x\n",
    "                    \n",
    "                    submask[min_y:max_y+1, min_x:max_x+1] = convex_hull_image(submask[min_y:max_y+1, min_x:max_x+1])\n",
    "                    \n",
    "                    mask = mask + submask\n",
    "                        \n",
    "                #print(\"Filling contours\")\n",
    "                #mask = fill_contours(mask)\n",
    "                \n",
    "\n",
    "                #mask = np.packbits(mask,axis=None)\n",
    "                #print(\"Binarized mask\")\n",
    "                \n",
    "                print(\"Saving mask contours\")\n",
    "                np.save(MASK_DIR+NAID+'.npy',mask) \n",
    "\n",
    "                print(\"processed in \", time.time()-start,\" seconds\")\n",
    "                print(\"____________________________________________\")\n",
    "            else:\n",
    "                print(\"Error in generating masks from polygon for \", NAID)\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        print(\"Skipped,\", imagename, '. This file is either not .czi or .svs, or not the file assigned')\n",
    "    \n",
    "    if wsi_gt == 1:\n",
    "        try:\n",
    "            del vips_img,coords,mask,submask\n",
    "            gc.collect()\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814175dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### SEGMENTATION - Find BG using ../saved_models/patched_512_NewAnnotation+preprocess_ResNet18_Feb12.pt (in 224)\n",
    "\n",
    "MODEL_SEG_DIR = './patched_512_NewAnnotation+preprocess_ResNet18_Feb12.pt'\n",
    "\n",
    "seg_model = torchvision.models.resnet18()\n",
    "seg_model.fc = nn.Linear(512, 3)\n",
    "\n",
    "checkpoint = torch.load(MODEL_SEG_DIR)\n",
    "seg_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "seg_model.to(device)\n",
    "\n",
    "\n",
    "for NAID in os.listdir(SAVE_DIR):\n",
    "    os.makedirs(SEGMENTATION_TILE_DIR+NAID)\n",
    "    os.makedirs(SEGMENTATION_TILE_DIR+NAID+'/BG/')\n",
    "    for tile_folder in os.listdir(SAVE_DIR+NAID+'/0/'):\n",
    "        for tile in os.listdir(SAVE_DIR+NAID+'/0/'+tile_folder):\n",
    "            seg_model.train(False)\n",
    "            \n",
    "            img = Image.open(SAVE_DIR+NAID+'/0/'+tile_folder+'/'+tile)\n",
    "            \n",
    "            img_tensor = transforms.ToTensor()(img)\n",
    "            img_tensor = transforms.Normalize(mean=[0.4409763317567454, 0.4016568471536302, 0.4988298669112181],\n",
    "                             std=[0.31297803931100737, 0.2990562933047881, 0.33747493782548915])(img_tensor)\n",
    "            img_tensor = torch.reshape(img_tensor,(1,3,512,512))\n",
    "            img_tensor = img_tensor.cuda()\n",
    "            \n",
    "            predict = seg_model(img_tensor)\n",
    "            preds = F.sigmoid(predict)\n",
    "            _, indices = torch.max(predict.data, 1) # indices = 0:Background, 1:WM, 2:GM\n",
    "            indices = indices.type(torch.uint8)\n",
    "            running_seg =  indices.data.cpu()\n",
    "            \n",
    "            \n",
    "            if running_seg == 0:\n",
    "                shutil.copyfile(SAVE_DIR+NAID+'/0/'+tile_folder+'/'+tile, SEGMENTATION_TILE_DIR+NAID+'/BG/'+tile)\n",
    "            else:\n",
    "                pass\n",
    "                #print(\"bg removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7c162f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### SEGMENTATION - Separate tiles into BG, Inf and Healthy folders\n",
    "gt_df = pd.read_csv('/cache/S23_Infarct/gt.csv')\n",
    "\n",
    "for imagename in tqdm(imagenames[:]):\n",
    "    start = time.time()\n",
    "\n",
    "    wsi_gt = gt_df[gt_df.cases == imagename].iat[0,2]\n",
    "    \n",
    "    NAID = imagename.split('.')[0]\n",
    "    print(\"Loading\", imagename, \" ......\")\n",
    "\n",
    "    os.makedirs(SEGMENTATION_TILE_DIR+NAID+'/Heal/')\n",
    "    \n",
    "    if wsi_gt == 1:\n",
    "        os.makedirs(SEGMENTATION_TILE_DIR+NAID+'/Inf/')\n",
    "\n",
    "    mask = np.load(MASK_DIR+NAID+'.npy')\n",
    "    print(\"Loaded mask at \", MASK_DIR+NAID+'.npy' ,\", about to process...\")\n",
    "\n",
    "    #mask = np.unpackbits(mask,count=im_size).reshape(img_dim).view(bool)\n",
    "    #mask = mask.view(np.uint8)\n",
    "    #print(\"Unbinarized mask\")\n",
    "\n",
    "    for tile_folder in os.listdir(SAVE_DIR+NAID+'/0/'):\n",
    "        for tile in os.listdir(SAVE_DIR+NAID+'/0/'+tile_folder):\n",
    "\n",
    "            #file naming convention --> y0 x0 y1 x1\n",
    "            y0 = tile.split('_')[0]\n",
    "            x0 = tile.split('_')[1]\n",
    "            y1 = tile.split('_')[2]\n",
    "            x1 = tile.split('_')[3]\n",
    "\n",
    "            if wsi_gt == 1:\n",
    "                if sum(sum(mask[y0:y1,x0:x1])) >= ((512*512)/2):\n",
    "                    shutil.copyfile(SAVE_DIR+NAID+'/0/'+tile_folder+'/'+tile, SEGMENTATION_TILE_DIR+NAID+'/Inf/'+tile)\n",
    "                elif sum(sum(mask[y0:y1,x0:x1])) == 0:\n",
    "                    shutil.copyfile(SAVE_DIR+NAID+'/0/'+tile_folder+'/'+tile, SEGMENTATION_TILE_DIR+NAID+'/Heal/'+tile)\n",
    "                else:\n",
    "                    pass\n",
    "            else:\n",
    "                shutil.copyfile(SAVE_DIR+NAID+'/0/'+tile_folder+'/'+tile, SEGMENTATION_TILE_DIR+NAID+'/Heal/'+tile)\n",
    "    \n",
    "    print(\"processed in \", time.time()-start,\" seconds\")\n",
    "    print(\"____________________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291bc9c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
